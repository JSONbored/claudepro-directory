{
  "slug": "session-health-score",
  "description": "Claude Code session health aggregator providing A-F grade based on cost efficiency, latency performance, productivity velocity, and cache utilization with actionable recommendations.",
  "category": "statuslines",
  "author": "JSONbored",
  "dateAdded": "2025-10-25",
  "tags": [
    "health-score",
    "session-grade",
    "performance-aggregator",
    "quality-metrics",
    "optimization-recommendations"
  ],
  "statuslineType": "custom",
  "content": "#!/usr/bin/env bash\n\n# Session Health Score for Claude Code\n# Aggregates multiple metrics into overall health grade (A-F)\n\n# Read JSON from stdin\nread -r input\n\n# Extract metrics\ntotal_cost=$(echo \"$input\" | jq -r '.cost.total_cost_usd // 0')\ntotal_duration_ms=$(echo \"$input\" | jq -r '.cost.total_duration_ms // 1')\napi_duration_ms=$(echo \"$input\" | jq -r '.cost.total_api_duration_ms // 0')\nlines_added=$(echo \"$input\" | jq -r '.cost.total_lines_added // 0')\nlines_removed=$(echo \"$input\" | jq -r '.cost.total_lines_removed // 0')\ncache_read_tokens=$(echo \"$input\" | jq -r '.cost.cache_read_input_tokens // 0')\ncache_create_tokens=$(echo \"$input\" | jq -r '.cost.cache_creation_input_tokens // 0')\nregular_input_tokens=$(echo \"$input\" | jq -r '.cost.input_tokens // 0')\n\n# Convert duration to minutes\nif [ \"$total_duration_ms\" -gt 0 ]; then\n  duration_minutes=$(echo \"scale=2; $total_duration_ms / 60000\" | bc)\nelse\n  duration_minutes=0.01\nfi\n\n# METRIC 1: Cost Efficiency (25 points)\n# Target: <$0.05/min = excellent, $0.05-$0.10 = good, >$0.10 = poor\nif (( $(echo \"$duration_minutes > 0\" | bc -l) )); then\n  cost_per_minute=$(echo \"scale=4; $total_cost / $duration_minutes\" | bc)\nelse\n  cost_per_minute=0\nfi\n\nif (( $(echo \"$cost_per_minute < 0.05\" | bc -l) )); then\n  cost_score=25\nelif (( $(echo \"$cost_per_minute < 0.10\" | bc -l) )); then\n  cost_score=18\nelif (( $(echo \"$cost_per_minute < 0.20\" | bc -l) )); then\n  cost_score=12\nelse\n  cost_score=5\nfi\n\n# METRIC 2: Latency Performance (25 points)\n# Target: network time <1s = excellent, 1-3s = good, >3s = poor\nnetwork_time=$((total_duration_ms - api_duration_ms))\nnetwork_seconds=$(echo \"scale=2; $network_time / 1000\" | bc)\n\nif (( $(echo \"$network_seconds < 1\" | bc -l) )); then\n  latency_score=25\nelif (( $(echo \"$network_seconds < 3\" | bc -l) )); then\n  latency_score=18\nelif (( $(echo \"$network_seconds < 5\" | bc -l) )); then\n  latency_score=12\nelse\n  latency_score=5\nfi\n\n# METRIC 3: Productivity Velocity (25 points)\n# Target: >30 lines/min = excellent, 15-30 = good, <15 = poor\ntotal_changed=$((lines_added + lines_removed))\nif (( $(echo \"$duration_minutes > 0\" | bc -l) )); then\n  lines_per_minute=$(echo \"scale=1; $total_changed / $duration_minutes\" | bc)\nelse\n  lines_per_minute=0\nfi\n\nif (( $(echo \"$lines_per_minute > 30\" | bc -l) )); then\n  productivity_score=25\nelif (( $(echo \"$lines_per_minute > 15\" | bc -l) )); then\n  productivity_score=18\nelif (( $(echo \"$lines_per_minute > 5\" | bc -l) )); then\n  productivity_score=12\nelse\n  productivity_score=5\nfi\n\n# METRIC 4: Cache Utilization (25 points)\n# Target: >40% hit rate = excellent, 20-40% = good, <20% = poor\ntotal_input=$((cache_read_tokens + cache_create_tokens + regular_input_tokens))\nif [ $total_input -gt 0 ]; then\n  cache_hit_rate=$(( (cache_read_tokens * 100) / total_input ))\nelse\n  cache_hit_rate=0\nfi\n\nif [ $cache_hit_rate -ge 40 ]; then\n  cache_score=25\nelif [ $cache_hit_rate -ge 20 ]; then\n  cache_score=18\nelif [ $cache_hit_rate -gt 0 ]; then\n  cache_score=12\nelse\n  cache_score=5  # No caching detected\nfi\n\n# Calculate total health score (0-100)\nhealth_score=$((cost_score + latency_score + productivity_score + cache_score))\n\n# Assign letter grade\nif [ $health_score -ge 90 ]; then\n  GRADE=\"A\"\n  GRADE_COLOR=\"\\033[38;5;46m\"   # Green\n  GRADE_ICON=\"ðŸŒŸ\"\n  STATUS=\"EXCELLENT\"\nelif [ $health_score -ge 80 ]; then\n  GRADE=\"B\"\n  GRADE_COLOR=\"\\033[38;5;75m\"   # Blue\n  GRADE_ICON=\"âœ“\"\n  STATUS=\"GOOD\"\nelif [ $health_score -ge 70 ]; then\n  GRADE=\"C\"\n  GRADE_COLOR=\"\\033[38;5;226m\"  # Yellow\n  GRADE_ICON=\"â—\"\n  STATUS=\"AVERAGE\"\nelif [ $health_score -ge 60 ]; then\n  GRADE=\"D\"\n  GRADE_COLOR=\"\\033[38;5;208m\"  # Orange\n  GRADE_ICON=\"âš \"\n  STATUS=\"BELOW AVG\"\nelse\n  GRADE=\"F\"\n  GRADE_COLOR=\"\\033[38;5;196m\"  # Red\n  GRADE_ICON=\"âœ—\"\n  STATUS=\"POOR\"\nfi\n\n# Identify weakest metric for recommendation\nweakest_score=$cost_score\nweakest_metric=\"cost\"\n\nif [ $latency_score -lt $weakest_score ]; then\n  weakest_score=$latency_score\n  weakest_metric=\"latency\"\nfi\n\nif [ $productivity_score -lt $weakest_score ]; then\n  weakest_score=$productivity_score\n  weakest_metric=\"productivity\"\nfi\n\nif [ $cache_score -lt $weakest_score ]; then\n  weakest_score=$cache_score\n  weakest_metric=\"cache\"\nfi\n\n# Actionable recommendation based on weakest metric\ncase \"$weakest_metric\" in\n  cost)\n    RECOMMENDATION=\"ðŸ’¡ Optimize: Switch to Haiku for simple tasks\"\n    ;;\n  latency)\n    RECOMMENDATION=\"ðŸ’¡ Optimize: Check network connection\"\n    ;;\n  productivity)\n    RECOMMENDATION=\"ðŸ’¡ Optimize: Increase automation/prompts\"\n    ;;\n  cache)\n    RECOMMENDATION=\"ðŸ’¡ Optimize: Enable prompt caching\"\n    ;;\nesac\n\n# Show recommendation only if grade is C or below\nif [ $health_score -lt 70 ]; then\n  SHOW_REC=\"$RECOMMENDATION\"\nelse\n  SHOW_REC=\"\"\nfi\n\nRESET=\"\\033[0m\"\n\n# Build metric breakdown (abbreviated)\nMETRICS=\"C:${cost_score} L:${latency_score} P:${productivity_score} K:${cache_score}\"\n\n# Output statusline\nif [ -n \"$SHOW_REC\" ]; then\n  echo -e \"${GRADE_ICON} Health: ${GRADE_COLOR}${GRADE}${RESET} (${health_score}/100) | ${METRICS} | ${SHOW_REC}\"\nelse\n  echo -e \"${GRADE_ICON} Health: ${GRADE_COLOR}${GRADE}${RESET} (${health_score}/100) | ${STATUS} | ${METRICS}\"\nfi\n",
  "features": [
    "Comprehensive health score aggregating 4 key metrics into A-F grade (90+ = A, 80-89 = B, 70-79 = C, 60-69 = D, <60 = F)",
    "Cost efficiency metric (25 points): burn rate vs target <$0.05/min",
    "Latency performance metric (25 points): network time vs target <1s",
    "Productivity velocity metric (25 points): lines/min vs target >30 L/min",
    "Cache utilization metric (25 points): hit rate vs target >40%",
    "Actionable recommendations identifying weakest metric with optimization tips",
    "Metric breakdown display showing individual scores (C:25 L:18 P:12 K:25)",
    "Color-coded grading (green A, blue B, yellow C, orange D, red F)"
  ],
  "configuration": {
    "format": "bash",
    "refreshInterval": 2000,
    "position": "left"
  },
  "useCases": [
    "Quick session quality assessment at a glance",
    "Identifying performance bottlenecks across multiple dimensions",
    "Comparing session health across different projects/workflows",
    "Optimizing Claude Code usage based on weakest metric feedback",
    "Team performance benchmarking with standardized grading",
    "Historical session quality tracking for improvement trends"
  ],
  "requirements": [
    "Bash shell",
    "jq JSON processor",
    "bc calculator (for floating-point arithmetic)"
  ],
  "preview": "âœ“ Health: B (82/100) | GOOD | C:25 L:18 P:22 K:17",
  "troubleshooting": [
    {
      "issue": "Health score always showing F grade despite good session",
      "solution": "Verify all JSON fields exist: cost.total_cost_usd, cost.total_duration_ms, cost.total_api_duration_ms, cost.total_lines_added, cost.total_lines_removed, cost.cache_read_input_tokens. Missing fields default to 0, causing low scores. Check: echo '$input' | jq .cost. Ensure Claude Code version exposes all required metrics."
    },
    {
      "issue": "Individual metric scores seem incorrect",
      "solution": "Check thresholds: Cost (<$0.05/min = 25pt, <$0.10 = 18pt), Latency (<1s network = 25pt, <3s = 18pt), Productivity (>30 L/min = 25pt, >15 = 18pt), Cache (>40% hit = 25pt, >20% = 18pt). Verify calculations: cost_per_minute = total_cost / duration_minutes. Test each metric independently."
    },
    {
      "issue": "Recommendation not showing despite low grade",
      "solution": "Recommendations only display for grades C (70-79), D (60-69), or F (<60). Grades A (90+) and B (80-89) show STATUS instead. Check health_score value and comparison: if [ $health_score -lt 70 ]. If score is exactly 70, triggers C grade but no recommendation (boundary condition)."
    },
    {
      "issue": "Weakest metric identification incorrect",
      "solution": "Script compares cost_score, latency_score, productivity_score, cache_score to find minimum. Check individual scores: echo C=$cost_score L=$latency_score P=$productivity_score K=$cache_score. Verify comparison logic uses -lt (less than). If tied, first metric in order wins (cost > latency > productivity > cache)."
    },
    {
      "issue": "Cache score always showing 5 despite caching enabled",
      "solution": "Cache score requires cache_read_input_tokens field. Verify: echo '$input' | jq .cost.cache_read_input_tokens. If missing/null, defaults to 0 giving minimum score (5pt). Check that prompt caching is properly configured and Claude Code version supports cache metrics. Zero cache_read_tokens = no cache benefit detected."
    },
    {
      "issue": "Metric breakdown abbreviations confusing",
      "solution": "Abbreviations: C = Cost efficiency, L = Latency performance, P = Productivity velocity, K = Cache utilization (K for Kache to avoid confusion with C). Each shows score out of 25 points. Add legend to documentation or customize METRICS string for clarity."
    }
  ],
  "documentationUrl": "https://docs.claude.com/en/docs/claude-code/statusline",
  "source": "community",
  "discoveryMetadata": {
    "researchDate": "2025-10-25",
    "trendingSources": [
      {
        "source": "anthropic_official_docs",
        "evidence": "Official docs confirm JSON provides all required fields: cost metrics, duration metrics, lines changed, cache tokens. Statuslines can aggregate data for composite scores.",
        "url": "https://docs.claude.com/en/docs/claude-code/statusline",
        "relevanceScore": "high"
      },
      {
        "source": "dev_to_performance_monitoring",
        "evidence": "2025 developer productivity guides emphasize composite health scores combining multiple dimensions (cost, latency, output). Single-metric dashboards miss optimization opportunities.",
        "url": "https://dev.to/",
        "relevanceScore": "high"
      },
      {
        "source": "hackernews",
        "evidence": "HackerNews discussions on developer metrics highlight need for holistic session quality assessment. A-F grading simplifies complex multi-metric analysis for quick decision-making.",
        "url": "https://news.ycombinator.com/",
        "relevanceScore": "high"
      },
      {
        "source": "reddit_programming",
        "evidence": "Reddit threads on AI tool optimization: 'Need single dashboard view of session health'. Aggregating cost, latency, productivity, caching into one score addresses common pain point.",
        "url": "https://www.reddit.com/r/programming/",
        "relevanceScore": "medium"
      }
    ],
    "keywordResearch": {
      "primaryKeywords": [
        "session health score",
        "performance grade tracker",
        "holistic monitoring",
        "quality aggregator"
      ],
      "searchVolume": "medium",
      "competitionLevel": "low"
    },
    "gapAnalysis": {
      "existingContent": [
        "burn-rate-monitor",
        "api-latency-breakdown",
        "lines-per-minute-tracker",
        "cache-efficiency-monitor"
      ],
      "identifiedGap": "Existing statuslines track individual metrics in isolation (cost, latency, productivity, cache). No AGGREGATION into holistic health score. Users must mentally combine multiple statuslines to assess overall session quality. Single A-F grade with actionable recommendations is completely missing.",
      "priority": "high"
    },
    "approvalRationale": "Official docs verified all required fields available for aggregation. Dev.to/HN/Reddit validate demand for composite health scoring. Medium search volume, low competition. Clear gap vs single-metric trackers (no holistic view). Critical for simplifying multi-dimensional optimization. User approved for session health scoring."
  }
}
