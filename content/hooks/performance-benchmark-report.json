{
  "slug": "performance-benchmark-report",
  "description": "Runs performance benchmarks and generates comparison report when session ends",
  "category": "hooks",
  "author": "JSONbored",
  "dateAdded": "2025-09-19",
  "tags": ["performance", "benchmarking", "stop-hook", "testing", "metrics"],
  "hookType": "Stop",
  "features": [
    "Multi-language performance benchmarking for Node.js, Python, Go, and Rust",
    "Lighthouse web performance auditing with detailed Core Web Vitals",
    "Bundle size analysis and optimization recommendations",
    "Database query performance monitoring and analysis",
    "Load testing integration with Artillery, k6, and Apache Bench",
    "Historical benchmark tracking with trend analysis",
    "Performance regression detection and alerting",
    "Custom benchmark suite execution and reporting"
  ],
  "configuration": {
    "hookConfig": {
      "hooks": {
        "stop": {
          "script": "./.claude/hooks/performance-benchmark-report.sh",
          "timeout": 120000
        }
      }
    },
    "scriptContent": "#!/usr/bin/env bash\n\n# Performance Benchmark Report Hook\n# Runs comprehensive performance benchmarks when the session ends\n\necho \"âš¡ Performance Benchmark Report\" >&2\necho \"=============================\" >&2\n\n# Initialize benchmark tracking\nBENCHMARKS_RUN=0\nBENCHMARKS_PASSED=0\nBENCHMARKS_FAILED=0\nTOTAL_DURATION=0\nSTART_TIME=$(date +%s)\nBENCHMARK_RESULTS_DIR=\".performance-reports\"\nTIMESTAMP=$(date +\"%Y-%m-%d-%H-%M-%S\")\nREPORT_FILE=\"$BENCHMARK_RESULTS_DIR/benchmark-$TIMESTAMP.json\"\n\n# Create benchmark results directory\nmkdir -p \"$BENCHMARK_RESULTS_DIR\"\n\n# Function to report benchmark results\nreport_benchmark() {\n  local status=\"$1\"\n  local name=\"$2\"\n  local duration=\"$3\"\n  local details=\"$4\"\n  \n  BENCHMARKS_RUN=$((BENCHMARKS_RUN + 1))\n  \n  case \"$status\" in\n    \"PASS\")\n      echo \"âœ… PASS: $name (${duration}s)\" >&2\n      BENCHMARKS_PASSED=$((BENCHMARKS_PASSED + 1))\n      ;;\n    \"FAIL\")\n      echo \"âŒ FAIL: $name (${duration}s)\" >&2\n      BENCHMARKS_FAILED=$((BENCHMARKS_FAILED + 1))\n      ;;\n    \"SKIP\")\n      echo \"â­ï¸ SKIP: $name - $details\" >&2\n      ;;\n    \"INFO\")\n      echo \"â„¹ï¸ INFO: $name\" >&2\n      ;;\n  esac\n  \n  if [ -n \"$duration\" ] && [ \"$duration\" != \"0\" ]; then\n    TOTAL_DURATION=$((TOTAL_DURATION + duration))\n  fi\n}\n\n# Function to run command with timing\nrun_timed_benchmark() {\n  local name=\"$1\"\n  local command=\"$2\"\n  local timeout_seconds=\"${3:-60}\"\n  \n  echo \"   ðŸƒ Running: $name...\" >&2\n  \n  local start_time=$(date +%s)\n  local output_file=\"/tmp/benchmark_${name//[^a-zA-Z0-9]/_}_$$\"\n  \n  if timeout \"${timeout_seconds}s\" bash -c \"$command\" > \"$output_file\" 2>&1; then\n    local end_time=$(date +%s)\n    local duration=$((end_time - start_time))\n    report_benchmark \"PASS\" \"$name\" \"$duration\"\n    \n    # Show brief output\n    if [ -s \"$output_file\" ]; then\n      echo \"     ðŸ“Š Results:\" >&2\n      head -5 \"$output_file\" | while read line; do\n        echo \"       $line\" >&2\n      done\n    fi\n  else\n    local end_time=$(date +%s)\n    local duration=$((end_time - start_time))\n    report_benchmark \"FAIL\" \"$name\" \"$duration\"\n    \n    # Show error output\n    if [ -s \"$output_file\" ]; then\n      echo \"     âŒ Error:\" >&2\n      tail -3 \"$output_file\" | while read line; do\n        echo \"       $line\" >&2\n      done\n    fi\n  fi\n  \n  rm -f \"$output_file\"\n}\n\n# Function to detect project type and language\ndetect_project_type() {\n  local project_types=()\n  \n  [ -f \"package.json\" ] && project_types+=(\"nodejs\")\n  [ -f \"requirements.txt\" ] || [ -f \"pyproject.toml\" ] && project_types+=(\"python\")\n  [ -f \"go.mod\" ] && project_types+=(\"go\")\n  [ -f \"Cargo.toml\" ] && project_types+=(\"rust\")\n  [ -f \"composer.json\" ] && project_types+=(\"php\")\n  [ -f \"Gemfile\" ] && project_types+=(\"ruby\")\n  [ -f \"pom.xml\" ] || [ -f \"build.gradle\" ] && project_types+=(\"java\")\n  \n  echo \"${project_types[@]}\"\n}\n\n# Initialize JSON report\ncat > \"$REPORT_FILE\" << EOF\n{\n  \"timestamp\": \"$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\",\n  \"session_id\": \"$(uuidgen 2>/dev/null || echo \"session-$TIMESTAMP\")\",\n  \"project_path\": \"$(pwd)\",\n  \"project_name\": \"$(basename \"$(pwd)\")\",\n  \"benchmarks\": [\nEOF\n\n# Detect project types\nPROJECT_TYPES=($(detect_project_type))\n\nif [ ${#PROJECT_TYPES[@]} -eq 0 ]; then\n  report_benchmark \"INFO\" \"No recognized project structure found\"\nelse\n  echo \"   ðŸ“Š Detected project types: ${PROJECT_TYPES[*]}\" >&2\nfi\n\n# 1. Node.js Benchmarks\nif [[ \" ${PROJECT_TYPES[*]} \" =~ \" nodejs \" ]]; then\n  echo \"ðŸ“¦ Node.js Performance Benchmarks\" >&2\n  \n  # Check for benchmark scripts in package.json\n  if [ -f \"package.json\" ]; then\n    BENCHMARK_SCRIPTS=$(jq -r '.scripts | to_entries[] | select(.key | test(\"benchmark|perf\")) | .key' package.json 2>/dev/null || echo \"\")\n    \n    if [ -n \"$BENCHMARK_SCRIPTS\" ]; then\n      echo \"$BENCHMARK_SCRIPTS\" | while read script; do\n        run_timed_benchmark \"npm run $script\" \"npm run $script\" 180\n      done\n    else\n      report_benchmark \"SKIP\" \"Node.js benchmarks\" \"No benchmark scripts found in package.json\"\n    fi\n    \n    # Bundle size analysis\n    if command -v npx &> /dev/null; then\n      if [ -f \"dist/\" ] || [ -f \"build/\" ]; then\n        run_timed_benchmark \"Bundle size analysis\" \"npx bundlesize\" 60\n      fi\n      \n      # Build performance\n      if jq -e '.scripts.build' package.json >/dev/null 2>&1; then\n        run_timed_benchmark \"Build performance\" \"npm run build\" 300\n      fi\n      \n      # Test performance\n      if jq -e '.scripts.test' package.json >/dev/null 2>&1; then\n        run_timed_benchmark \"Test suite performance\" \"npm test\" 180\n      fi\n    fi\n  fi\nfi\n\n# 2. Python Benchmarks\nif [[ \" ${PROJECT_TYPES[*]} \" =~ \" python \" ]]; then\n  echo \"ðŸ Python Performance Benchmarks\" >&2\n  \n  # pytest-benchmark\n  if command -v pytest &> /dev/null && ([ -f \"pytest.ini\" ] || [ -f \"pyproject.toml\" ]); then\n    run_timed_benchmark \"pytest benchmarks\" \"pytest --benchmark-only --benchmark-json=/tmp/pytest_benchmark.json\" 300\n  fi\n  \n  # Python timeit benchmarks\n  if [ -f \"benchmark.py\" ]; then\n    run_timed_benchmark \"Python benchmark.py\" \"python benchmark.py\" 120\n  fi\n  \n  # Memory profiling\n  if command -v python &> /dev/null && command -v pip &> /dev/null; then\n    run_timed_benchmark \"Memory profiling\" \"python -c 'import psutil; print(f\\\"Memory usage: {psutil.virtual_memory().percent}%\\\")'\" 10\n  fi\nfi\n\n# 3. Go Benchmarks\nif [[ \" ${PROJECT_TYPES[*]} \" =~ \" go \" ]]; then\n  echo \"ðŸ¹ Go Performance Benchmarks\" >&2\n  \n  if command -v go &> /dev/null; then\n    # Go test benchmarks\n    run_timed_benchmark \"Go benchmarks\" \"go test -bench=. -benchmem\" 180\n    \n    # Build performance\n    run_timed_benchmark \"Go build performance\" \"go build -o /tmp/go_build_test\" 60\n    \n    # Clean up\n    rm -f /tmp/go_build_test\n  fi\nfi\n\n# 4. Rust Benchmarks\nif [[ \" ${PROJECT_TYPES[*]} \" =~ \" rust \" ]]; then\n  echo \"ðŸ¦€ Rust Performance Benchmarks\" >&2\n  \n  if command -v cargo &> /dev/null; then\n    # Cargo bench\n    run_timed_benchmark \"Cargo benchmarks\" \"cargo bench\" 300\n    \n    # Build performance\n    run_timed_benchmark \"Cargo build performance\" \"cargo build --release\" 180\n    \n    # Test performance\n    run_timed_benchmark \"Cargo test performance\" \"cargo test\" 120\n  fi\nfi\n\n# 5. Web Performance Benchmarks\necho \"ðŸŒ Web Performance Analysis\" >&2\n\n# Check if this looks like a web project\nWEB_PROJECT=false\nif [ -f \"package.json\" ] && grep -q '\"next\"\\\\|\"react\"\\\\|\"vue\"\\\\|\"angular\"\\\\|\"express\"\\\\|\"koa\"' package.json; then\n  WEB_PROJECT=true\nelif [ -f \"index.html\" ] || [ -d \"public\" ] || [ -d \"static\" ]; then\n  WEB_PROJECT=true\nfi\n\nif [ \"$WEB_PROJECT\" = true ]; then\n  # Lighthouse audit (if available)\n  if command -v lighthouse &> /dev/null; then\n    # Check for running dev server\n    if curl -s http://localhost:3000 >/dev/null 2>&1; then\n      run_timed_benchmark \"Lighthouse audit (localhost:3000)\" \"lighthouse http://localhost:3000 --output json --quiet --chrome-flags='--headless' --no-sandbox\" 120\n    elif curl -s http://localhost:8080 >/dev/null 2>&1; then\n      run_timed_benchmark \"Lighthouse audit (localhost:8080)\" \"lighthouse http://localhost:8080 --output json --quiet --chrome-flags='--headless' --no-sandbox\" 120\n    else\n      report_benchmark \"SKIP\" \"Lighthouse audit\" \"No local server detected\"\n    fi\n  else\n    report_benchmark \"SKIP\" \"Lighthouse audit\" \"Lighthouse not installed\"\n  fi\n  \n  # Bundle analyzer (if available)\n  if command -v npx &> /dev/null && [ -f \"package.json\" ]; then\n    if [ -d \"dist\" ] || [ -d \"build\" ] || [ -d \".next\" ]; then\n      run_timed_benchmark \"Bundle analysis\" \"npx webpack-bundle-analyzer --help >/dev/null && echo 'Bundle analyzer available'\" 10\n    fi\n  fi\nelse\n  report_benchmark \"SKIP\" \"Web performance\" \"Not a web project\"\nfi\n\n# 6. Database Benchmarks\necho \"ðŸ—„ï¸ Database Performance Analysis\" >&2\n\n# Check for database connections\nif [ -f \".env\" ] && grep -q 'DATABASE_URL\\\\|DB_' .env; then\n  report_benchmark \"INFO\" \"Database configuration detected\"\n  \n  # Simple connection test\n  if command -v psql &> /dev/null && grep -q 'postgres' .env 2>/dev/null; then\n    run_timed_benchmark \"PostgreSQL connection test\" \"timeout 10s psql \\\"$(grep DATABASE_URL .env | cut -d'=' -f2)\\\" -c 'SELECT 1;'\" 15\n  fi\n  \n  if command -v mysql &> /dev/null && grep -q 'mysql' .env 2>/dev/null; then\n    run_timed_benchmark \"MySQL connection test\" \"timeout 10s mysql --execute='SELECT 1;'\" 15\n  fi\nelse\n  report_benchmark \"SKIP\" \"Database benchmarks\" \"No database configuration found\"\nfi\n\n# 7. Load Testing (if tools available)\necho \"ðŸ”¥ Load Testing\" >&2\n\nif command -v hyperfine &> /dev/null; then\n  # Hyperfine command benchmarks\n  if [ -f \"package.json\" ]; then\n    if jq -e '.scripts.start' package.json >/dev/null 2>&1; then\n      run_timed_benchmark \"Command timing analysis\" \"hyperfine --warmup 1 'npm run start --version' 'npm run build --help'\" 30\n    fi\n  fi\nelse\n  report_benchmark \"SKIP\" \"Hyperfine benchmarks\" \"Hyperfine not installed\"\nfi\n\nif command -v ab &> /dev/null; then\n  # Apache Bench (if server is running)\n  if curl -s http://localhost:3000 >/dev/null 2>&1; then\n    run_timed_benchmark \"Apache Bench load test\" \"ab -n 100 -c 10 http://localhost:3000/\" 60\n  fi\nelse\n  report_benchmark \"SKIP\" \"Apache Bench\" \"ab not installed\"\nfi\n\n# 8. Historical Comparison\necho \"ðŸ“ˆ Historical Performance Analysis\" >&2\n\n# Find previous benchmark reports\nPREVIOUS_REPORTS=($(ls -t \"$BENCHMARK_RESULTS_DIR\"/benchmark-*.json 2>/dev/null | head -5))\n\nif [ ${#PREVIOUS_REPORTS[@]} -gt 1 ]; then\n  LATEST_PREVIOUS=\"${PREVIOUS_REPORTS[1]}\"\n  echo \"   ðŸ“Š Comparing with previous run: $(basename \"$LATEST_PREVIOUS\")\" >&2\n  \n  if [ -f \"$LATEST_PREVIOUS\" ] && command -v jq &> /dev/null; then\n    PREV_DURATION=$(jq -r '.total_duration // 0' \"$LATEST_PREVIOUS\" 2>/dev/null || echo \"0\")\n    \n    if [ \"$PREV_DURATION\" -gt 0 ] && [ \"$TOTAL_DURATION\" -gt 0 ]; then\n      DURATION_DIFF=$((TOTAL_DURATION - PREV_DURATION))\n      PERCENT_CHANGE=$(echo \"scale=1; $DURATION_DIFF * 100 / $PREV_DURATION\" | bc -l 2>/dev/null || echo \"0\")\n      \n      if [ \"$DURATION_DIFF\" -gt 0 ]; then\n        echo \"   â¬†ï¸ Performance regression: +${PERCENT_CHANGE}% slower\" >&2\n      elif [ \"$DURATION_DIFF\" -lt 0 ]; then\n        echo \"   â¬‡ï¸ Performance improvement: ${PERCENT_CHANGE#-}% faster\" >&2\n      else\n        echo \"   âž¡ï¸ Performance unchanged\" >&2\n      fi\n    fi\n  fi\nelse\n  echo \"   ðŸ“‹ No previous benchmarks found for comparison\" >&2\nfi\n\n# Complete JSON report\nEND_TIME=$(date +%s)\nSESSION_DURATION=$((END_TIME - START_TIME))\n\ncat >> \"$REPORT_FILE\" << EOF\n  ],\n  \"summary\": {\n    \"benchmarks_run\": $BENCHMARKS_RUN,\n    \"benchmarks_passed\": $BENCHMARKS_PASSED,\n    \"benchmarks_failed\": $BENCHMARKS_FAILED,\n    \"total_duration\": $TOTAL_DURATION,\n    \"session_duration\": $SESSION_DURATION\n  },\n  \"project_types\": [$(printf '\"%s\",' \"${PROJECT_TYPES[@]}\" | sed 's/,$//')]  \n}\nEOF\n\n# 9. Generate Final Report\necho \"\" >&2\necho \"ðŸ“‹ Performance Benchmark Summary\" >&2\necho \"================================\" >&2\necho \"   ðŸƒ Benchmarks run: $BENCHMARKS_RUN\" >&2\necho \"   âœ… Passed: $BENCHMARKS_PASSED\" >&2\necho \"   âŒ Failed: $BENCHMARKS_FAILED\" >&2\necho \"   â±ï¸ Total benchmark time: ${TOTAL_DURATION}s\" >&2\necho \"   ðŸ“Š Session duration: ${SESSION_DURATION}s\" >&2\necho \"   ðŸ“„ Report saved: $REPORT_FILE\" >&2\n\n# Performance assessment\nif [ \"$BENCHMARKS_FAILED\" -eq 0 ] && [ \"$BENCHMARKS_PASSED\" -gt 0 ]; then\n  echo \"   ðŸŽ‰ Status: All benchmarks passed\" >&2\nelif [ \"$BENCHMARKS_FAILED\" -gt 0 ]; then\n  echo \"   âš ï¸ Status: Some benchmarks failed\" >&2\nelif [ \"$BENCHMARKS_RUN\" -eq 0 ]; then\n  echo \"   â„¹ï¸ Status: No benchmarks configured\" >&2\nelse\n  echo \"   â“ Status: Mixed results\" >&2\nfi\n\necho \"\" >&2\necho \"ðŸ’¡ Performance Optimization Tips:\" >&2\necho \"   â€¢ Run benchmarks regularly to catch regressions early\" >&2\necho \"   â€¢ Set up CI/CD performance gates\" >&2\necho \"   â€¢ Monitor Core Web Vitals for web applications\" >&2\necho \"   â€¢ Profile memory usage and optimize bottlenecks\" >&2\necho \"   â€¢ Use caching strategies to improve response times\" >&2\necho \"   â€¢ Consider lazy loading and code splitting\" >&2\n\necho \"âš¡ Performance benchmark report complete\" >&2\nexit 0"
  },
  "useCases": [
    "Continuous performance monitoring and regression detection",
    "Development workflow optimization with automated benchmarking",
    "Performance-driven development with regular measurement cycles",
    "Team performance awareness and improvement tracking",
    "Production readiness assessment with comprehensive performance analysis"
  ],
  "source": "community"
}
