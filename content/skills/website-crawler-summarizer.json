{
  "slug": "website-crawler-summarizer",
  "title": "Website Content Crawler and Summarizer",
  "seoTitle": "Website Crawler + Summarizer Skill",
  "description": "Crawl domains respectfully, extract readable content, dedupe, and generate structured summaries.",
  "category": "skills",
  "author": "JSONbored",
  "dateAdded": "2025-10-15",
  "tags": ["crawler", "scraping", "summarization", "readability"],
  "content": "# Website Crawler & Summarizer Skill\n\n## What This Skill Enables\n\nClaude can crawl websites, extract content from web pages, clean HTML to readable text, respect robots.txt, and generate structured summaries or documentation from web content. Perfect for research, competitive analysis, and content aggregation.\n\n## Prerequisites\n\n**Required:**\n- Claude Pro subscription\n- Code Interpreter feature enabled\n- URLs to crawl (or sitemap)\n\n**What Claude handles:**\n- Installing scraping libraries (BeautifulSoup, Playwright, Scrapy)\n- Fetching web pages\n- Parsing HTML\n- Extracting readable content\n- Respecting robots.txt\n- Rate limiting requests\n- Content deduplication\n\n## How to Use This Skill\n\n### Single Page Extraction\n\n**Prompt:** \"Extract the main content from this webpage: https://example.com/article\nConvert to clean Markdown and save as article.md\"\n\nClaude will:\n1. Fetch the webpage\n2. Parse HTML\n3. Extract main content (remove ads, nav, footer)\n4. Convert to Markdown\n5. Save clean output\n\n### Multi-Page Crawl\n\n**Prompt:** \"Crawl all pages linked from this site:\nStart URL: https://docs.example.com\nOnly crawl pages under /docs/\nExtract content from each page\nSave as individual Markdown files\nMax 50 pages\"\n\nClaude will:\n1. Fetch start URL\n2. Find all links\n3. Filter to matching paths\n4. Crawl each page (with rate limiting)\n5. Extract and save content\n6. Generate index of pages\n\n### Content Summarization\n\n**Prompt:** \"Crawl these 10 blog posts and:\n1. Extract main content from each\n2. Generate 2-sentence summary per post\n3. Identify key topics\n4. Create master summary document\nFormat as JSON with metadata.\"\n\nClaude will:\n1. Fetch all URLs\n2. Extract content\n3. Generate summaries\n4. Extract topics/tags\n5. Structure as JSON\n\n### Sitemap-Based Crawl\n\n**Prompt:** \"Download sitemap from https://example.com/sitemap.xml and:\n1. Extract all article URLs\n2. Crawl each article\n3. Extract: title, date, author, content\n4. Save as CSV with metadata\"\n\nClaude will:\n1. Fetch sitemap XML\n2. Parse URL list\n3. Crawl each URL (respecting rate limits)\n4. Extract structured data\n5. Export as CSV\n\n## Common Workflows\n\n### Competitor Analysis\n```\n\"Analyze competitor website:\n1. Crawl main site (max 20 pages)\n2. Extract: services, pricing mentions, features\n3. Identify key messaging themes\n4. Create structured comparison report\nFocus on product/service pages.\"\n```\n\n### Documentation Mirror\n```\n\"Create local mirror of documentation:\n1. Start at https://docs.example.com\n2. Crawl all /docs/* pages\n3. Download images referenced\n4. Convert to Markdown\n5. Preserve link structure\n6. Generate offline-browseable site\"\n```\n\n### Research Aggregation\n```\n\"Gather research from these 20 URLs:\n1. Extract main content from each\n2. Identify key findings and quotes\n3. Extract citations and references\n4. Group by topic/theme\n5. Create annotated bibliography\nOutput as structured Markdown.\"\n```\n\n### Change Detection\n```\n\"Monitor this webpage for changes:\n1. Fetch current version\n2. Extract main content\n3. Compare with version from last week\n4. Highlight what changed\n5. Generate change report\"\n```\n\n## Web Scraping Best Practices\n\n### Respect & Ethics\n- **Always check robots.txt**: Claude will respect crawl rules\n- **Rate limiting**: Default to 1-2 requests/second\n- **User agent**: Identify bot politely\n- **Terms of service**: Respect website ToS\n- **Copyright**: Content remains property of original creator\n\n### Technical Considerations\n- **Dynamic content**: Use Playwright for JavaScript-heavy sites\n- **Authentication**: Provide cookies/tokens if needed\n- **Pagination**: Handle \"Load More\" and infinite scroll\n- **Anti-bot measures**: Respect CAPTCHAs (don't try to bypass)\n\n## Content Extraction Methods\n\n### HTML Parsing\n- BeautifulSoup for static HTML\n- CSS selectors for targeting elements\n- XPath for complex queries\n\n### Readability Algorithms\n- Remove boilerplate (nav, ads, footers)\n- Extract main article content\n- Preserve formatting (headings, lists, links)\n\n### Structured Data\n- JSON-LD extraction\n- Schema.org metadata\n- Open Graph tags\n- Twitter Cards\n\n## Tips for Best Results\n\n1. **Start Small**: Test with 1-2 pages before bulk crawling\n2. **Specify Scope**: Define which pages to crawl (\"only /blog/* paths\")\n3. **Rate Limits**: Mention if you need slower crawling (\"1 page per 5 seconds\")\n4. **Content Type**: Describe what to extract (\"article text only, no comments\")\n5. **Error Handling**: \"Skip pages that error and continue\" vs \"stop on first error\"\n6. **Deduplication**: \"Skip duplicate content\" if crawling related pages\n7. **Storage**: Specify output format (Markdown, JSON, CSV, HTML)\n\n## Advanced Features\n\n### JavaScript Rendering\n- Use Playwright for SPAs\n- Wait for dynamic content to load\n- Handle infinite scroll\n- Click \"Load More\" buttons\n\n### Link Discovery\n- Find all links on page\n- Filter by pattern (regex)\n- Depth-limited crawling\n- Breadth-first vs depth-first\n\n### Data Extraction\n- Tables to CSV\n- Lists to arrays\n- Forms and inputs\n- Metadata extraction\n\n### Content Processing\n- HTML to Markdown conversion\n- Text cleaning and normalization\n- Language detection\n- Content summarization\n\n## Troubleshooting\n\n**Issue:** Getting blocked or rate limited\n**Solution:** \"Slow down to 1 request per 10 seconds\" and \"Add random delays between requests\"\n\n**Issue:** Content not extracting correctly\n**Solution:** \"Show me the raw HTML first\" then identify CSS selectors for main content\n\n**Issue:** JavaScript content not loading\n**Solution:** \"Use Playwright to render JavaScript\" or \"Wait 5 seconds for content to load\"\n\n**Issue:** Too many pages being crawled\n**Solution:** Set limits: \"Max 50 pages\" or \"Only crawl 2 levels deep\" or \"Stick to /docs/* path\"\n\n**Issue:** Images/assets not downloading\n**Solution:** \"Download all images referenced in articles\" or provide specific asset types needed\n\n**Issue:** Different page structures\n**Solution:** Provide multiple CSS selectors: \"Try article.content, then div.post-body, then main\"\n\n## Legal & Ethical Considerations\n\n**Important**: Always respect:\n- Copyright and intellectual property\n- Website terms of service\n- robots.txt directives\n- Rate limits and server resources\n- Privacy and personal data\n- Commercial use restrictions\n\n**Use cases**: Research, archival, accessibility, personal use\n**Prohibited**: Spam, unauthorized scraping, data theft, ToS violations\n\n## Learn More\n\n- [robots.txt Guide](https://developers.google.com/search/docs/crawling-indexing/robots/intro) - Crawling etiquette\n- [BeautifulSoup Docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - HTML parsing\n- [Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html) - Advanced crawling\n- [Playwright](https://playwright.dev/) - Browser automation\n- [Mozilla Readability](https://github.com/mozilla/readability) - Content extraction\n",
  "features": [
    "Robots-aware crawling",
    "Boilerplate removal",
    "Language detection",
    "JSON/MD export"
  ],
  "useCases": ["Competitive intel packs", "Documentation mirrors", "Research briefs"],
  "requirements": [
    "Node.js 18+ or Python 3.11+",
    "Playwright (optional)",
    "readability or newspaper3k"
  ],
  "examples": [
    {
      "title": "Basic fetch + Readability (Node)",
      "language": "javascript",
      "code": "import { JSDOM } from 'jsdom';\nimport { Readability } from '@mozilla/readability';\nimport fetch from 'node-fetch';\n\nconst html = await (await fetch('https://example.com')).text();\nconst doc = new JSDOM(html, { url: 'https://example.com' });\nconst article = new Readability(doc.window.document).parse();\nconsole.log(article.title);"
    }
  ],
  "installation": {
    "claudeDesktop": {
      "steps": ["Install Node.js 18+", "npm i jsdom @mozilla/readability node-fetch"]
    },
    "claudeCode": { "steps": ["Configure rate-limit and user-agent", "Respect robots.txt"] }
  },
  "troubleshooting": [
    {
      "issue": "Blocked by anti-bot",
      "solution": "Reduce concurrency, add polite delays, and avoid sensitive endpoints."
    },
    {
      "issue": "BeautifulSoup returning None for elements that visibly exist on page",
      "solution": "Content may be JavaScript-rendered. Use Playwright or Puppeteer to render DOM. Check if element is in iframe or shadow DOM."
    },
    {
      "issue": "UnicodeDecodeError or mojibake characters in extracted content",
      "solution": "Detect charset from HTTP headers or meta tags. Use response.encoding='utf-8' or chardet library. Specify parser='lxml' explicitly."
    },
    {
      "issue": "Playwright crawl timing out or consuming excessive memory with multiple pages",
      "solution": "Close browser contexts after each page. Use browser.newContext() per session. Set timeout limits and enable headless mode."
    },
    {
      "issue": "Readability extraction missing article content or extracting wrong sections",
      "solution": "Try different parsers (lxml, html.parser, html5lib). Manually specify article CSS selector as fallback if auto-detect fails."
    }
  ],
  "documentationUrl": "https://github.com/mozilla/readability",
  "source": "community"
}
