{
  "slug": "cloudflare-workers-ai-edge",
  "title": "Cloudflare Workers AI Edge Functions",
  "seoTitle": "Cloudflare Workers AI Edge Functions Skill",
  "description": "Deploy AI models and serverless functions to Cloudflare's global edge network with sub-5ms cold starts and 40% edge computing market share.",
  "category": "skills",
  "author": "JSONbored",
  "dateAdded": "2025-10-16",
  "tags": ["cloudflare", "edge-computing", "ai", "serverless", "workers"],
  "content": "# Cloudflare Workers AI Edge Functions Skill\n\n## What This Skill Enables\n\nClaude can build and deploy AI-powered serverless functions on Cloudflare's global edge network, spanning 275+ cities with sub-5ms cold start times (10-80x faster than AWS Lambda@Edge). With 40% edge computing market share and 4,000% year-over-year growth in AI inference requests, Cloudflare Workers AI brings machine learning models directly to users worldwide with minimal latency.\n\n## Prerequisites\n\n**Required:**\n- Claude Pro subscription or Claude Code CLI\n- Cloudflare account (free tier available)\n- Wrangler CLI installed (`npm install -g wrangler`)\n- Basic understanding of JavaScript/TypeScript\n\n**What Claude handles automatically:**\n- Writing Workers code with TypeScript types\n- Configuring wrangler.toml for deployments\n- Implementing AI model bindings (Llama-2, Whisper, Stable Diffusion)\n- Setting up D1 database and R2 storage integrations\n- Managing environment variables and secrets\n- Deploying to Cloudflare's edge network\n- Optimizing for V8 isolate performance\n\n## How to Use This Skill\n\n### Deploy a Basic Edge Function\n\n**Prompt:** \"Create a Cloudflare Worker that responds to HTTP requests with JSON data and deploys to the edge.\"\n\nClaude will:\n1. Generate a Worker with proper `fetch` event handler\n2. Create `wrangler.toml` configuration\n3. Set up TypeScript types for Request/Response\n4. Add error handling and CORS headers\n5. Deploy with `wrangler publish`\n6. Provide the deployed Worker URL\n\n### AI Model Integration (Llama-2 Chat)\n\n**Prompt:** \"Build a Cloudflare Worker that uses Llama-2 to generate chat responses. Accept POST requests with user messages and stream the AI responses back.\"\n\nClaude will:\n1. Configure AI binding in wrangler.toml\n2. Implement streaming response with ReadableStream\n3. Add proper prompt formatting for Llama-2\n4. Set up rate limiting to control costs\n5. Include request validation and error handling\n6. Deploy with Workers AI binding enabled\n\n### Image Generation with Stable Diffusion\n\n**Prompt:** \"Create an edge function that generates images using Stable Diffusion XL. Accept a text prompt via API and return the generated image URL stored in R2.\"\n\nClaude will:\n1. Set up Workers AI binding for Stable Diffusion\n2. Configure R2 bucket for image storage\n3. Implement image generation with proper parameters\n4. Upload generated images to R2 with public URLs\n5. Add caching headers for CDN optimization\n6. Include usage analytics with D1 database\n\n### Real-Time Translation API\n\n**Prompt:** \"Build a translation API using Cloudflare Workers AI that detects the source language and translates to the target language. Support 50+ languages with edge caching.\"\n\nClaude will:\n1. Use Workers AI translation models\n2. Implement language detection\n3. Set up KV namespace for translation caching\n4. Add rate limiting per IP address\n5. Configure CDN cache for common translations\n6. Include usage metrics and error logging\n\n## Tips for Best Results\n\n1. **Leverage V8 Isolates**: Workers use V8 isolates that start in <5ms and use 1/10th the memory of Node.js. Design stateless functions that take advantage of this architecture.\n\n2. **Use Durable Objects for State**: For stateful operations (WebSockets, real-time collaboration), request Durable Objects implementation instead of external databases.\n\n3. **Model Selection**: Choose appropriate AI models based on latency requirements. Smaller models like Llama-2-7B offer faster inference than larger variants.\n\n4. **Edge Caching**: Implement Cache API or KV storage for frequently accessed data to reduce AI inference costs.\n\n5. **Cost Optimization**: Workers AI charges per request. Use caching, rate limiting, and request batching to optimize costs.\n\n6. **Geographic Routing**: Workers automatically route to the nearest data center. For AI models, consider pinning specific regions for data residency compliance.\n\n## Common Workflows\n\n### Full-Stack AI Application\n```\n\"Create a complete AI-powered application on Cloudflare:\n1. Workers AI for text generation (Llama-2)\n2. D1 database for storing conversations\n3. R2 for file uploads and generated content\n4. KV for session management and caching\n5. Pages for frontend deployment\n6. Queue for background job processing\nInclude TypeScript types and deployment scripts.\"\n```\n\n### Content Moderation API\n```\n\"Build an edge API that:\n1. Accepts text content via POST request\n2. Uses Workers AI to detect harmful content\n3. Classifies content as safe/unsafe with confidence scores\n4. Logs results to D1 database\n5. Returns moderation decision in <100ms\n6. Handles 10,000 requests per minute\"\n```\n\n### Smart Image CDN\n```\n\"Create a Cloudflare Worker that:\n1. Intercepts image requests\n2. Analyzes image with Workers AI (OCR, object detection)\n3. Automatically optimizes images for device/bandwidth\n4. Stores optimized versions in R2\n5. Serves from edge cache on subsequent requests\n6. Includes usage analytics and cost tracking\"\n```\n\n### Real-Time Sentiment Analysis\n```\n\"Build a WebSocket-based sentiment analysis service:\n1. Accept streaming text via WebSocket\n2. Process chunks with Workers AI sentiment model\n3. Return real-time sentiment scores\n4. Store aggregate results in D1\n5. Support 1000 concurrent connections\n6. Deploy across all Cloudflare edge locations\"\n```\n\n## Troubleshooting\n\n**Issue:** Worker exceeds CPU time limits\n**Solution:** Workers have a 50ms CPU time limit on free tier (30s on paid). Optimize by using streaming responses, reducing synchronous processing, or upgrading to Unbound workers for longer execution.\n\n**Issue:** AI model inference too slow\n**Solution:** Use smaller model variants (e.g., Llama-2-7B instead of 13B), implement request queuing with Workers Queue, or cache common responses in KV storage.\n\n**Issue:** CORS errors when calling from frontend\n**Solution:** Add proper CORS headers in Worker response. Ask Claude to include OPTIONS method handler and appropriate Access-Control-* headers.\n\n**Issue:** Workers AI billing concerns\n**Solution:** Implement rate limiting with Durable Objects or KV, cache responses aggressively, use smaller models for simpler tasks, and set up billing alerts in Cloudflare dashboard.\n\n**Issue:** Cannot access environment variables\n**Solution:** Ensure secrets are set with `wrangler secret put` and bindings are properly configured in wrangler.toml. Access via `env.SECRET_NAME` in Worker code.\n\n**Issue:** Cold start latency for complex Workers\n**Solution:** Minimize dependencies (Workers bundle size should be <1MB), use dynamic imports for optional features, and consider splitting into multiple Workers for different routes.\n\n## Learn More\n\n- [Cloudflare Workers AI Documentation](https://developers.cloudflare.com/workers-ai/)\n- [Workers AI Models Catalog](https://developers.cloudflare.com/workers-ai/models/)\n- [Wrangler CLI Guide](https://developers.cloudflare.com/workers/wrangler/)\n- [Workers Platform Architecture](https://blog.cloudflare.com/cloud-computing-without-containers/)\n- [Edge Computing Best Practices](https://developers.cloudflare.com/workers/learning/how-workers-works/)\n- [Durable Objects Guide](https://developers.cloudflare.com/durable-objects/)\n",
  "features": [
    "Sub-5ms cold starts with V8 isolates",
    "20+ AI models: Llama-2, Whisper, Stable Diffusion",
    "Deploy to 275+ cities globally",
    "Integrated with D1, R2, KV, Queues"
  ],
  "useCases": [
    "Edge AI inference with minimal latency",
    "Serverless APIs with global distribution",
    "Real-time content moderation and analysis"
  ],
  "requirements": [
    "Cloudflare account",
    "Wrangler CLI 3.0+",
    "Node.js 18+",
    "@cloudflare/workers-types"
  ],
  "examples": [
    {
      "title": "AI Chat Worker with Llama-2",
      "language": "typescript",
      "code": "export interface Env {\n  AI: any;\n}\n\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    if (request.method !== 'POST') {\n      return new Response('Method not allowed', { status: 405 });\n    }\n\n    const { messages } = await request.json<{ messages: any[] }>();\n\n    const response = await env.AI.run('@cf/meta/llama-2-7b-chat-int8', {\n      messages: [\n        { role: 'system', content: 'You are a helpful assistant.' },\n        ...messages,\n      ],\n      stream: true,\n    });\n\n    return new Response(response, {\n      headers: {\n        'content-type': 'text/event-stream',\n        'cache-control': 'no-cache',\n      },\n    });\n  },\n};"
    },
    {
      "title": "Image Generation with Stable Diffusion + R2",
      "language": "typescript",
      "code": "export interface Env {\n  AI: any;\n  IMAGES: R2Bucket;\n}\n\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    const { prompt } = await request.json<{ prompt: string }>();\n\n    // Generate image with Stable Diffusion\n    const response = await env.AI.run(\n      '@cf/stabilityai/stable-diffusion-xl-base-1.0',\n      { prompt }\n    );\n\n    // Upload to R2\n    const imageKey = `${crypto.randomUUID()}.png`;\n    await env.IMAGES.put(imageKey, response, {\n      httpMetadata: { contentType: 'image/png' },\n    });\n\n    const imageUrl = `https://images.example.com/${imageKey}`;\n\n    return Response.json({\n      success: true,\n      imageUrl,\n      prompt,\n    });\n  },\n};"
    },
    {
      "title": "Translation API with KV Caching",
      "language": "typescript",
      "code": "export interface Env {\n  AI: any;\n  TRANSLATIONS: KVNamespace;\n}\n\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    const { text, targetLang = 'en' } = await request.json<{\n      text: string;\n      targetLang: string;\n    }>();\n\n    // Check cache first\n    const cacheKey = `${text}:${targetLang}`;\n    const cached = await env.TRANSLATIONS.get(cacheKey);\n    if (cached) {\n      return Response.json({ translation: cached, cached: true });\n    }\n\n    // Translate with Workers AI\n    const response = await env.AI.run('@cf/meta/m2m100-1.2b', {\n      text,\n      target_lang: targetLang,\n    });\n\n    const translation = response.translated_text;\n\n    // Cache for 24 hours\n    await env.TRANSLATIONS.put(cacheKey, translation, {\n      expirationTtl: 86400,\n    });\n\n    return Response.json({ translation, cached: false });\n  },\n};"
    },
    {
      "title": "wrangler.toml Configuration",
      "language": "toml",
      "code": "name = \"ai-worker\"\nmain = \"src/index.ts\"\ncompatibility_date = \"2025-10-16\"\n\n[ai]\nbinding = \"AI\"\n\n[[r2_buckets]]\nbinding = \"IMAGES\"\nbucket_name = \"my-images\"\n\n[[kv_namespaces]]\nbinding = \"TRANSLATIONS\"\nid = \"your-kv-namespace-id\"\n\n[[d1_databases]]\nbinding = \"DB\"\ndatabase_name = \"my-database\"\ndatabase_id = \"your-database-id\"\n\n[observability]\nenabled = true"
    }
  ],
  "installation": {
    "claudeDesktop": {
      "steps": [
        "Install Wrangler: npm install -g wrangler",
        "Login to Cloudflare: wrangler login",
        "Ask Claude: 'Create a Cloudflare Worker with AI capabilities'",
        "Claude generates code and deploys with wrangler publish"
      ]
    },
    "claudeCode": {
      "steps": [
        "npm install -g wrangler",
        "wrangler login",
        "wrangler init my-worker",
        "Add AI binding to wrangler.toml",
        "wrangler publish"
      ]
    }
  },
  "troubleshooting": [
    {
      "issue": "Worker exceeds size limit",
      "solution": "Minimize dependencies, use dynamic imports, or split into multiple Workers. Bundle size should be <1MB for optimal performance."
    },
    {
      "issue": "AI binding not available",
      "solution": "Ensure [ai] binding is configured in wrangler.toml and account has Workers AI enabled."
    },
    {
      "issue": "R2 upload fails",
      "solution": "Verify R2 bucket binding in wrangler.toml and ensure bucket exists in Cloudflare dashboard."
    }
  ],
  "documentationUrl": "https://developers.cloudflare.com/workers-ai/",
  "source": "community"
}
