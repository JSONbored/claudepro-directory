{
  "description": "Advanced performance optimization with bottleneck analysis, memory profiling, and automated improvements",
  "category": "commands",
  "author": "claudepro",
  "dateAdded": "2025-09-16",
  "tags": ["performance", "optimization", "profiling", "bottleneck", "efficiency"],
  "content": "The `/optimize` command provides comprehensive performance analysis and optimization recommendations including bottleneck identification, memory profiling, algorithm improvements, and automated code transformations.\n\n## Usage\n\n```\n/optimize [options] <file_or_function>\n```\n\n## Options\n\n### Optimization Types\n- `--performance` - CPU and execution time optimization\n- `--memory` - Memory usage and allocation optimization\n- `--network` - Network request and bandwidth optimization\n- `--database` - Database query and connection optimization\n- `--bundle` - Bundle size and loading optimization\n- `--all` - Comprehensive optimization analysis (default)\n\n### Analysis Depth\n- `--quick` - Fast analysis with basic recommendations\n- `--detailed` - Comprehensive profiling and analysis\n- `--deep` - Advanced algorithm and architecture analysis\n- `--benchmark` - Performance benchmarking and comparison\n\n### Target Metrics\n- `--latency` - Focus on response time reduction\n- `--throughput` - Focus on request handling capacity\n- `--scalability` - Focus on scaling characteristics\n- `--efficiency` - Focus on resource utilization\n\n### Output Options\n- `--format=report` - Detailed optimization report (default)\n- `--format=diff` - Before/after code comparison\n- `--format=metrics` - Performance metrics and benchmarks\n- `--format=interactive` - Interactive optimization guide\n\n## Examples\n\n### Database Query Optimization\n\n```javascript\n// Unoptimized code with multiple performance issues\nclass ProductService {\n  constructor(database) {\n    this.db = database;\n  }\n  \n  // üêå Issue 1: N+1 Query Problem\n  async getProductsWithReviews() {\n    const products = await this.db.query('SELECT * FROM products');\n    \n    for (const product of products) {\n      // üêå Executes N queries (one per product)\n      product.reviews = await this.db.query(\n        'SELECT * FROM reviews WHERE product_id = ?', \n        [product.id]\n      );\n      \n      // üêå Issue 2: Another N queries for user data\n      for (const review of product.reviews) {\n        review.user = await this.db.query(\n          'SELECT name, avatar FROM users WHERE id = ?',\n          [review.user_id]\n        );\n      }\n    }\n    \n    return products;\n  }\n  \n  // üêå Issue 3: Inefficient search without indexes\n  async searchProducts(searchTerm) {\n    return await this.db.query(`\n      SELECT * FROM products \n      WHERE LOWER(name) LIKE LOWER('%${searchTerm}%') \n         OR LOWER(description) LIKE LOWER('%${searchTerm}%')\n      ORDER BY name\n    `);\n  }\n  \n  // üêå Issue 4: No pagination, loads all data\n  async getPopularProducts() {\n    return await this.db.query(`\n      SELECT p.*, COUNT(r.id) as review_count,\n             AVG(r.rating) as avg_rating\n      FROM products p\n      LEFT JOIN reviews r ON p.id = r.product_id\n      GROUP BY p.id\n      ORDER BY review_count DESC, avg_rating DESC\n    `);\n  }\n  \n  // üêå Issue 5: Expensive aggregation on every call\n  async getProductStats(productId) {\n    const product = await this.db.query(\n      'SELECT * FROM products WHERE id = ?', \n      [productId]\n    );\n    \n    const reviewCount = await this.db.query(\n      'SELECT COUNT(*) as count FROM reviews WHERE product_id = ?',\n      [productId]\n    );\n    \n    const avgRating = await this.db.query(\n      'SELECT AVG(rating) as avg FROM reviews WHERE product_id = ?',\n      [productId]\n    );\n    \n    const recentReviews = await this.db.query(\n      'SELECT * FROM reviews WHERE product_id = ? ORDER BY created_at DESC LIMIT 5',\n      [productId]\n    );\n    \n    return {\n      ...product[0],\n      reviewCount: reviewCount[0].count,\n      avgRating: avgRating[0].avg,\n      recentReviews\n    };\n  }\n}\n```\n\n**Optimization Analysis:**\n\n```markdown\n# üöÄ Performance Optimization Report\n\n## üìä Performance Issues Identified\n\n### Issue 1: N+1 Query Problem (Critical)\n**Location:** `getProductsWithReviews()` method\n**Impact:** üî¥ Severe - O(n¬≤) database queries\n**Current Performance:** 1,000 products = 2,001 queries\n**Estimated Fix Impact:** 99.5% query reduction\n\n**Problem Analysis:**\n```\nCurrent Execution:\n1. SELECT * FROM products (1 query)\n2. For each product (N queries):\n   - SELECT * FROM reviews WHERE product_id = ?\n3. For each review (N*M queries):\n   - SELECT name, avatar FROM users WHERE id = ?\n\nTotal Queries: 1 + N + (N * avg_reviews_per_product)\nWith 100 products, 5 reviews each: 1 + 100 + 500 = 601 queries!\n```\n\n**Optimized Solution:**\n```javascript\nasync getProductsWithReviews() {\n  // ‚úÖ Single optimized query with JOINs\n  const query = `\n    SELECT \n      p.id as product_id,\n      p.name as product_name,\n      p.description,\n      p.price,\n      p.created_at as product_created_at,\n      r.id as review_id,\n      r.rating,\n      r.comment,\n      r.created_at as review_created_at,\n      u.name as user_name,\n      u.avatar as user_avatar\n    FROM products p\n    LEFT JOIN reviews r ON p.id = r.product_id\n    LEFT JOIN users u ON r.user_id = u.id\n    ORDER BY p.id, r.created_at DESC\n  `;\n  \n  const rows = await this.db.query(query);\n  \n  // ‚úÖ Transform flat result into nested structure\n  const productsMap = new Map();\n  \n  for (const row of rows) {\n    if (!productsMap.has(row.product_id)) {\n      productsMap.set(row.product_id, {\n        id: row.product_id,\n        name: row.product_name,\n        description: row.description,\n        price: row.price,\n        created_at: row.product_created_at,\n        reviews: []\n      });\n    }\n    \n    const product = productsMap.get(row.product_id);\n    \n    if (row.review_id) {\n      product.reviews.push({\n        id: row.review_id,\n        rating: row.rating,\n        comment: row.comment,\n        created_at: row.review_created_at,\n        user: {\n          name: row.user_name,\n          avatar: row.user_avatar\n        }\n      });\n    }\n  }\n  \n  return Array.from(productsMap.values());\n}\n\n// ‚úÖ Performance improvement: 601 queries ‚Üí 1 query (99.8% reduction)\n```\n\n### Issue 2: Missing Database Indexes (High)\n**Location:** `searchProducts()` method\n**Impact:** üü° High - Full table scans on every search\n**Current Performance:** O(n) scan of entire products table\n**Estimated Fix Impact:** 10-100x search speed improvement\n\n**Index Recommendations:**\n```sql\n-- ‚úÖ Full-text search index for product names and descriptions\nCREATE FULLTEXT INDEX idx_products_search \nON products(name, description);\n\n-- ‚úÖ Composite index for filtered searches\nCREATE INDEX idx_products_category_price \nON products(category_id, price);\n\n-- ‚úÖ Index for popular products query\nCREATE INDEX idx_reviews_product_rating \nON reviews(product_id, rating);\n```\n\n**Optimized Search Query:**\n```javascript\nasync searchProducts(searchTerm, filters = {}) {\n  let query = `\n    SELECT p.*, \n           MATCH(p.name, p.description) AGAINST(? IN NATURAL LANGUAGE MODE) as relevance\n    FROM products p\n    WHERE MATCH(p.name, p.description) AGAINST(? IN NATURAL LANGUAGE MODE)\n  `;\n  \n  const params = [searchTerm, searchTerm];\n  \n  // ‚úÖ Add filters with indexed columns\n  if (filters.category_id) {\n    query += ' AND p.category_id = ?';\n    params.push(filters.category_id);\n  }\n  \n  if (filters.min_price) {\n    query += ' AND p.price >= ?';\n    params.push(filters.min_price);\n  }\n  \n  if (filters.max_price) {\n    query += ' AND p.price <= ?';\n    params.push(filters.max_price);\n  }\n  \n  query += ' ORDER BY relevance DESC, p.name LIMIT ? OFFSET ?';\n  params.push(filters.limit || 20, filters.offset || 0);\n  \n  return await this.db.query(query, params);\n}\n```\n\n### Issue 3: Missing Pagination (Medium)\n**Location:** `getPopularProducts()` method\n**Impact:** üü° Medium - Memory and bandwidth waste\n**Current Performance:** Loads entire dataset regardless of need\n**Estimated Fix Impact:** 80% memory reduction, faster response times\n\n**Optimized with Pagination:**\n```javascript\nasync getPopularProducts(page = 1, pageSize = 20) {\n  const offset = (page - 1) * pageSize;\n  \n  // ‚úÖ Paginated query with LIMIT/OFFSET\n  const [products, totalCount] = await Promise.all([\n    this.db.query(`\n      SELECT p.id, p.name, p.price, p.image_url,\n             COUNT(r.id) as review_count,\n             ROUND(AVG(r.rating), 2) as avg_rating\n      FROM products p\n      LEFT JOIN reviews r ON p.id = r.product_id\n      GROUP BY p.id\n      HAVING review_count > 0\n      ORDER BY review_count DESC, avg_rating DESC\n      LIMIT ? OFFSET ?\n    `, [pageSize, offset]),\n    \n    // ‚úÖ Get total count for pagination metadata\n    this.db.query(`\n      SELECT COUNT(DISTINCT p.id) as total\n      FROM products p\n      INNER JOIN reviews r ON p.id = r.product_id\n    `)\n  ]);\n  \n  return {\n    products,\n    pagination: {\n      page,\n      pageSize,\n      total: totalCount[0].total,\n      totalPages: Math.ceil(totalCount[0].total / pageSize)\n    }\n  };\n}\n```\n\n### Issue 4: Redundant Aggregation Queries (Medium)\n**Location:** `getProductStats()` method\n**Impact:** üü° Medium - Multiple unnecessary database roundtrips\n**Current Performance:** 4 separate queries per call\n**Estimated Fix Impact:** 75% query reduction\n\n**Optimized Single Query:**\n```javascript\nasync getProductStats(productId) {\n  // ‚úÖ Single query with all required data\n  const result = await this.db.query(`\n    SELECT \n      p.*,\n      COUNT(r.id) as review_count,\n      ROUND(AVG(r.rating), 2) as avg_rating,\n      JSON_ARRAYAGG(\n        CASE \n          WHEN r.id IS NOT NULL \n          THEN JSON_OBJECT(\n            'id', r.id,\n            'rating', r.rating,\n            'comment', r.comment,\n            'created_at', r.created_at,\n            'user_name', u.name\n          )\n          ELSE NULL\n        END\n      ) as recent_reviews\n    FROM products p\n    LEFT JOIN (\n      SELECT * FROM reviews \n      WHERE product_id = ? \n      ORDER BY created_at DESC \n      LIMIT 5\n    ) r ON p.id = r.product_id\n    LEFT JOIN users u ON r.user_id = u.id\n    WHERE p.id = ?\n    GROUP BY p.id\n  `, [productId, productId]);\n  \n  const product = result[0];\n  \n  // ‚úÖ Parse JSON array of recent reviews\n  product.recent_reviews = JSON.parse(product.recent_reviews)\n    .filter(review => review !== null);\n  \n  return product;\n}\n\n// ‚úÖ Performance improvement: 4 queries ‚Üí 1 query (75% reduction)\n```\n\n## üß† Caching Strategy Implementation\n\n```javascript\nconst Redis = require('redis');\n\nclass OptimizedProductService {\n  constructor(database, cache) {\n    this.db = database;\n    this.cache = cache || Redis.createClient();\n  }\n  \n  // ‚úÖ Multi-level caching strategy\n  async getProductStats(productId) {\n    const cacheKey = `product:stats:${productId}`;\n    \n    // Level 1: Memory cache check\n    let stats = this.memoryCache.get(cacheKey);\n    if (stats) {\n      return stats;\n    }\n    \n    // Level 2: Redis cache check\n    const cached = await this.cache.get(cacheKey);\n    if (cached) {\n      stats = JSON.parse(cached);\n      this.memoryCache.set(cacheKey, stats, 300); // 5 min memory cache\n      return stats;\n    }\n    \n    // Level 3: Database query\n    stats = await this.fetchProductStatsFromDB(productId);\n    \n    // Cache the result\n    await this.cache.setex(cacheKey, 3600, JSON.stringify(stats)); // 1 hour Redis cache\n    this.memoryCache.set(cacheKey, stats, 300); // 5 min memory cache\n    \n    return stats;\n  }\n  \n  // ‚úÖ Cache invalidation on updates\n  async updateProduct(productId, updates) {\n    await this.db.query(\n      'UPDATE products SET ? WHERE id = ?',\n      [updates, productId]\n    );\n    \n    // Invalidate related caches\n    await this.cache.del(`product:stats:${productId}`);\n    await this.cache.del(`product:${productId}`);\n    this.memoryCache.delete(`product:stats:${productId}`);\n  }\n}\n```\n\n## üìä Performance Benchmarks\n\n### Before Optimization\n```\nOperation                    | Time    | Queries | Memory\n---------------------------- | ------- | ------- | -------\ngetProductsWithReviews(100)  | 2.3s    | 601     | 45MB\nsearchProducts(\"laptop\")     | 450ms   | 1       | 12MB\ngetPopularProducts()         | 890ms   | 1       | 67MB\ngetProductStats(123)         | 180ms   | 4       | 2MB\n```\n\n### After Optimization\n```\nOperation                    | Time    | Queries | Memory  | Improvement\n---------------------------- | ------- | ------- | ------- | -----------\ngetProductsWithReviews(100)  | 45ms    | 1       | 8MB     | 98% faster\nsearchProducts(\"laptop\")     | 12ms    | 1       | 1MB     | 97% faster\ngetPopularProducts(20)       | 35ms    | 2       | 2MB     | 96% faster\ngetProductStats(123)         | 8ms     | 1       | 0.5MB   | 95% faster\n```\n\n### Load Testing Results\n```\nConcurrent Users: 1000\nTest Duration: 5 minutes\n\nBefore Optimization:\n‚îú‚îÄ Average Response Time: 1.2s\n‚îú‚îÄ 95th Percentile: 3.5s\n‚îú‚îÄ Requests/sec: 120\n‚îú‚îÄ Error Rate: 15%\n‚îî‚îÄ CPU Usage: 85%\n\nAfter Optimization:\n‚îú‚îÄ Average Response Time: 85ms\n‚îú‚îÄ 95th Percentile: 150ms\n‚îú‚îÄ Requests/sec: 2,400\n‚îú‚îÄ Error Rate: 0.1%\n‚îî‚îÄ CPU Usage: 25%\n\nImprovement:\n‚îú‚îÄ 14x faster response time\n‚îú‚îÄ 20x higher throughput\n‚îú‚îÄ 150x fewer errors\n‚îî‚îÄ 70% less CPU usage\n```\n\n## üîß Algorithm Optimization Examples\n\n### Array Processing Optimization\n\n```javascript\n// üêå Inefficient: Multiple array iterations\nfunction processProducts(products) {\n  // O(n) - Filter active products\n  const activeProducts = products.filter(p => p.status === 'active');\n  \n  // O(n) - Add discounted prices\n  const withDiscounts = activeProducts.map(p => ({\n    ...p,\n    discountedPrice: p.price * 0.9\n  }));\n  \n  // O(n) - Sort by price\n  const sorted = withDiscounts.sort((a, b) => a.discountedPrice - b.discountedPrice);\n  \n  // O(n) - Take first 10\n  return sorted.slice(0, 10);\n}\n\n// ‚úÖ Optimized: Single iteration with early termination\nfunction processProductsOptimized(products) {\n  const result = [];\n  \n  // O(n) but with early termination\n  for (const product of products) {\n    if (product.status !== 'active') continue;\n    \n    const processedProduct = {\n      ...product,\n      discountedPrice: product.price * 0.9\n    };\n    \n    // Insert in sorted position (for small arrays, faster than full sort)\n    insertSorted(result, processedProduct, (a, b) => a.discountedPrice - b.discountedPrice);\n    \n    // Early termination once we have enough results\n    if (result.length > 10) {\n      result.pop(); // Remove the most expensive item\n    }\n  }\n  \n  return result;\n}\n\nfunction insertSorted(array, item, compareFn) {\n  if (array.length === 0) {\n    array.push(item);\n    return;\n  }\n  \n  // Binary search for insertion point\n  let left = 0;\n  let right = array.length;\n  \n  while (left < right) {\n    const mid = Math.floor((left + right) / 2);\n    if (compareFn(array[mid], item) <= 0) {\n      left = mid + 1;\n    } else {\n      right = mid;\n    }\n  }\n  \n  array.splice(left, 0, item);\n}\n\n// Performance improvement: 4x faster for large datasets\n```\n\n### Memory-Efficient Data Processing\n\n```javascript\n// üêå Memory inefficient: Creates multiple intermediate arrays\nfunction processLargeDataset(data) {\n  return data\n    .filter(item => item.isValid)           // Creates copy 1\n    .map(item => transformItem(item))       // Creates copy 2\n    .filter(item => item.score > 0.5)       // Creates copy 3\n    .sort((a, b) => b.score - a.score)      // Modifies copy 3\n    .slice(0, 100);                         // Creates copy 4\n}\n\n// ‚úÖ Memory efficient: Generator-based streaming\nfunction* processLargeDatasetStream(data) {\n  const results = [];\n  \n  for (const item of data) {\n    if (!item.isValid) continue;\n    \n    const transformed = transformItem(item);\n    if (transformed.score <= 0.5) continue;\n    \n    // Insert in sorted position\n    insertSorted(results, transformed, (a, b) => b.score - a.score);\n    \n    // Keep only top 100\n    if (results.length > 100) {\n      results.pop();\n    }\n  }\n  \n  yield* results;\n}\n\n// Usage: Memory usage reduced by 80%\nconst results = Array.from(processLargeDatasetStream(largeDataset));\n```\n\n## üåê Network Optimization\n\n### API Request Batching\n\n```javascript\n// üêå Individual API requests\nclass UserService {\n  async getUsersWithProfiles(userIds) {\n    const users = [];\n    \n    for (const id of userIds) {\n      const user = await fetch(`/api/users/${id}`);\n      const profile = await fetch(`/api/profiles/${id}`);\n      \n      users.push({\n        ...await user.json(),\n        profile: await profile.json()\n      });\n    }\n    \n    return users;\n  }\n}\n\n// ‚úÖ Batched requests with concurrency control\nclass OptimizedUserService {\n  async getUsersWithProfiles(userIds) {\n    // Batch API requests\n    const batchSize = 10;\n    const batches = this.chunk(userIds, batchSize);\n    \n    const allResults = [];\n    \n    for (const batch of batches) {\n      // Parallel requests within batch\n      const [users, profiles] = await Promise.all([\n        this.batchFetchUsers(batch),\n        this.batchFetchProfiles(batch)\n      ]);\n      \n      // Combine results\n      const combined = users.map(user => ({\n        ...user,\n        profile: profiles.find(p => p.userId === user.id)\n      }));\n      \n      allResults.push(...combined);\n    }\n    \n    return allResults;\n  }\n  \n  async batchFetchUsers(ids) {\n    const response = await fetch('/api/users/batch', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ ids })\n    });\n    \n    return response.json();\n  }\n  \n  chunk(array, size) {\n    const chunks = [];\n    for (let i = 0; i < array.length; i += size) {\n      chunks.push(array.slice(i, i + size));\n    }\n    return chunks;\n  }\n}\n\n// Performance improvement: 10x faster for 100 users\n```\n\n### Request Deduplication\n\n```javascript\n// ‚úÖ Request deduplication to prevent duplicate API calls\nclass RequestCache {\n  constructor() {\n    this.cache = new Map();\n    this.pendingRequests = new Map();\n  }\n  \n  async get(url, options = {}) {\n    const key = this.generateKey(url, options);\n    \n    // Return cached result\n    if (this.cache.has(key)) {\n      return this.cache.get(key);\n    }\n    \n    // Join existing request if in progress\n    if (this.pendingRequests.has(key)) {\n      return this.pendingRequests.get(key);\n    }\n    \n    // Create new request\n    const request = this.fetchWithRetry(url, options)\n      .then(result => {\n        this.cache.set(key, result);\n        this.pendingRequests.delete(key);\n        \n        // Auto-expire cache\n        setTimeout(() => this.cache.delete(key), options.ttl || 300000);\n        \n        return result;\n      })\n      .catch(error => {\n        this.pendingRequests.delete(key);\n        throw error;\n      });\n    \n    this.pendingRequests.set(key, request);\n    return request;\n  }\n  \n  generateKey(url, options) {\n    return `${url}:${JSON.stringify(options.params || {})}`;\n  }\n  \n  async fetchWithRetry(url, options, retries = 3) {\n    for (let i = 0; i <= retries; i++) {\n      try {\n        const response = await fetch(url, options);\n        if (!response.ok) throw new Error(`HTTP ${response.status}`);\n        return await response.json();\n      } catch (error) {\n        if (i === retries) throw error;\n        await this.delay(Math.pow(2, i) * 1000); // Exponential backoff\n      }\n    }\n  }\n  \n  delay(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms));\n  }\n}\n\nconst apiCache = new RequestCache();\n\n// Usage: Automatic deduplication and caching\nconst users = await apiCache.get('/api/users/123');\n```\n\n## üì± Bundle Size Optimization\n\n### Code Splitting and Lazy Loading\n\n```javascript\n// üêå Large bundle: Everything loaded upfront\nimport React from 'react';\nimport { BrowserRouter, Routes, Route } from 'react-router-dom';\nimport HomePage from './pages/HomePage';\nimport ProductsPage from './pages/ProductsPage';\nimport UserProfilePage from './pages/UserProfilePage';\nimport AdminDashboard from './pages/AdminDashboard';\nimport ReportsPage from './pages/ReportsPage';\n\nfunction App() {\n  return (\n    <BrowserRouter>\n      <Routes>\n        <Route path=\"/\" element={<HomePage />} />\n        <Route path=\"/products\" element={<ProductsPage />} />\n        <Route path=\"/profile\" element={<UserProfilePage />} />\n        <Route path=\"/admin\" element={<AdminDashboard />} />\n        <Route path=\"/reports\" element={<ReportsPage />} />\n      </Routes>\n    </BrowserRouter>\n  );\n}\n\n// ‚úÖ Optimized: Lazy loading with code splitting\nimport React, { Suspense } from 'react';\nimport { BrowserRouter, Routes, Route } from 'react-router-dom';\n\n// Critical components loaded immediately\nimport HomePage from './pages/HomePage';\n\n// Non-critical components lazy loaded\nconst ProductsPage = React.lazy(() => import('./pages/ProductsPage'));\nconst UserProfilePage = React.lazy(() => import('./pages/UserProfilePage'));\nconst AdminDashboard = React.lazy(() => import('./pages/AdminDashboard'));\nconst ReportsPage = React.lazy(() => import('./pages/ReportsPage'));\n\nfunction App() {\n  return (\n    <BrowserRouter>\n      <Suspense fallback={<div className=\"loading\">Loading...</div>}>\n        <Routes>\n          <Route path=\"/\" element={<HomePage />} />\n          <Route path=\"/products\" element={<ProductsPage />} />\n          <Route path=\"/profile\" element={<UserProfilePage />} />\n          <Route path=\"/admin\" element={<AdminDashboard />} />\n          <Route path=\"/reports\" element={<ReportsPage />} />\n        </Routes>\n      </Suspense>\n    </BrowserRouter>\n  );\n}\n\n// Bundle size reduction: 60% smaller initial bundle\n```\n\n### Tree Shaking Optimization\n\n```javascript\n// üêå Imports entire lodash library\nimport _ from 'lodash';\n\nconst users = _.uniqBy(userList, 'id');\nconst sorted = _.sortBy(products, 'name');\n\n// ‚úÖ Optimized: Import only needed functions\nimport uniqBy from 'lodash/uniqBy';\nimport sortBy from 'lodash/sortBy';\n\nconst users = uniqBy(userList, 'id');\nconst sorted = sortBy(products, 'name');\n\n// Even better: Use native methods where possible\nconst users = userList.filter((user, index, array) => \n  array.findIndex(u => u.id === user.id) === index\n);\nconst sorted = products.sort((a, b) => a.name.localeCompare(b.name));\n\n// Bundle size reduction: 95% smaller (from 70KB to 3KB)\n```\n\n## üéØ Optimization Checklist\n\n### ‚úÖ Database Optimization\n- [ ] Identify and fix N+1 query problems\n- [ ] Add appropriate indexes for frequent queries\n- [ ] Implement query result caching\n- [ ] Use pagination for large datasets\n- [ ] Optimize JOIN operations and subqueries\n- [ ] Monitor slow query logs\n\n### ‚úÖ Memory Optimization\n- [ ] Identify memory leaks with profiling tools\n- [ ] Implement object pooling for frequent allocations\n- [ ] Use streaming for large data processing\n- [ ] Optimize data structures and algorithms\n- [ ] Implement garbage collection tuning\n\n### ‚úÖ Network Optimization\n- [ ] Implement request batching and deduplication\n- [ ] Add compression (gzip/brotli)\n- [ ] Use CDN for static assets\n- [ ] Implement HTTP/2 server push\n- [ ] Optimize API response sizes\n- [ ] Add retry logic with exponential backoff\n\n### ‚úÖ Frontend Optimization\n- [ ] Implement code splitting and lazy loading\n- [ ] Optimize bundle sizes with tree shaking\n- [ ] Use service workers for caching\n- [ ] Implement virtual scrolling for large lists\n- [ ] Optimize images and assets\n- [ ] Minimize render cycles with memoization\n\nThis optimization guide demonstrates systematic performance improvement with measurable results and best practices across all layers of the application stack.",
  "configuration": {
    "temperature": 0.2,
    "maxTokens": 16000,
    "systemPrompt": "You are a performance optimization expert with deep knowledge of algorithms, databases, caching strategies, and system architecture. Provide specific, measurable optimizations with before/after comparisons."
  },
  "githubUrl": "https://github.com/claudepro/performance-optimizer",
  "documentationUrl": "https://docs.claude.ai/commands/optimize",
  "source": "community",
  "slug": "optimize",
  "seoTitle": "Optimize for Claude"
}
